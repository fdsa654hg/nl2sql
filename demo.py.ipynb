{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fifth-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import cn2an\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils import read_data, read_tables, SQL, Query, Question, Table, RAdam\n",
    "from keras_bert import get_checkpoint_paths, load_vocabulary, Tokenizer, load_trained_model_from_checkpoint\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Lambda, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from utils import read_data, read_tables, SQL, Query, Question, Table\n",
    "\n",
    "from opencc import OpenCC\n",
    "cc = OpenCC('s2twp')\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "innocent-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table_file = './table.json'\n",
    "train_data_file = './t.json'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "model = BertModel.from_pretrained(\"hfl/chinese-bert-wwm\")\n",
    "\n",
    "train_tables = read_tables(train_table_file)\n",
    "train_data, train_max_len, train_max_header_len = read_data(train_data_file, train_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "center-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hungarian-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Train</th>\n",
       "      <th>BreastFeed</th>\n",
       "      <th>Route</th>\n",
       "      <th>Package</th>\n",
       "      <th>OverNightStn</th>\n",
       "      <th>LineDir</th>\n",
       "      <th>Line</th>\n",
       "      <th>Dinning</th>\n",
       "      <th>FoodSrv</th>\n",
       "      <th>...</th>\n",
       "      <th>Everyday</th>\n",
       "      <th>Note</th>\n",
       "      <th>NoteEng</th>\n",
       "      <th>Station</th>\n",
       "      <th>Order</th>\n",
       "      <th>DEPTime</th>\n",
       "      <th>ARRTime</th>\n",
       "      <th>ARRStation</th>\n",
       "      <th>ARRDEPTime</th>\n",
       "      <th>ARRARRTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1220</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>每日行駛。</td>\n",
       "      <td></td>\n",
       "      <td>樹林</td>\n",
       "      <td>1</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>16:18:00</td>\n",
       "      <td>浮洲</td>\n",
       "      <td>16:29:00</td>\n",
       "      <td>16:28:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1220</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>每日行駛。</td>\n",
       "      <td></td>\n",
       "      <td>樹林</td>\n",
       "      <td>1</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>16:18:00</td>\n",
       "      <td>板橋</td>\n",
       "      <td>16:33:00</td>\n",
       "      <td>16:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1220</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>每日行駛。</td>\n",
       "      <td></td>\n",
       "      <td>樹林</td>\n",
       "      <td>1</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>16:18:00</td>\n",
       "      <td>萬華</td>\n",
       "      <td>16:38:30</td>\n",
       "      <td>16:37:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1220</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>每日行駛。</td>\n",
       "      <td></td>\n",
       "      <td>樹林</td>\n",
       "      <td>1</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>16:18:00</td>\n",
       "      <td>臺北</td>\n",
       "      <td>16:45:00</td>\n",
       "      <td>16:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1220</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>每日行駛。</td>\n",
       "      <td></td>\n",
       "      <td>樹林</td>\n",
       "      <td>1</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>16:18:00</td>\n",
       "      <td>松山</td>\n",
       "      <td>16:52:00</td>\n",
       "      <td>16:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367159</th>\n",
       "      <td>1</td>\n",
       "      <td>4715</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>逢週六、日及例假日停駛。</td>\n",
       "      <td></td>\n",
       "      <td>瑞芳</td>\n",
       "      <td>9</td>\n",
       "      <td>11:06:00</td>\n",
       "      <td>11:05:00</td>\n",
       "      <td>暖暖</td>\n",
       "      <td>11:16:00</td>\n",
       "      <td>11:15:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367160</th>\n",
       "      <td>1</td>\n",
       "      <td>4715</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>逢週六、日及例假日停駛。</td>\n",
       "      <td></td>\n",
       "      <td>瑞芳</td>\n",
       "      <td>9</td>\n",
       "      <td>11:06:00</td>\n",
       "      <td>11:05:00</td>\n",
       "      <td>八堵</td>\n",
       "      <td>11:20:00</td>\n",
       "      <td>11:19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367161</th>\n",
       "      <td>1</td>\n",
       "      <td>4715</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>逢週六、日及例假日停駛。</td>\n",
       "      <td></td>\n",
       "      <td>四腳亭</td>\n",
       "      <td>10</td>\n",
       "      <td>11:12:00</td>\n",
       "      <td>11:11:00</td>\n",
       "      <td>暖暖</td>\n",
       "      <td>11:16:00</td>\n",
       "      <td>11:15:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367162</th>\n",
       "      <td>1</td>\n",
       "      <td>4715</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>逢週六、日及例假日停駛。</td>\n",
       "      <td></td>\n",
       "      <td>四腳亭</td>\n",
       "      <td>10</td>\n",
       "      <td>11:12:00</td>\n",
       "      <td>11:11:00</td>\n",
       "      <td>八堵</td>\n",
       "      <td>11:20:00</td>\n",
       "      <td>11:19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367163</th>\n",
       "      <td>1</td>\n",
       "      <td>4715</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>逢週六、日及例假日停駛。</td>\n",
       "      <td></td>\n",
       "      <td>暖暖</td>\n",
       "      <td>11</td>\n",
       "      <td>11:16:00</td>\n",
       "      <td>11:15:30</td>\n",
       "      <td>八堵</td>\n",
       "      <td>11:20:00</td>\n",
       "      <td>11:19:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367164 rows × 24 columns</p>\n",
       "</div><br>下午三點以後從臺北到高雄的火車有哪些？<br>sel: [19]<br>agg: ['']<br>cond_conn_op: 'and'<br>conds: [[17, '==', '臺北'], [19, '>', '15:00:00'], [21, '==', '高雄']]"
      ],
      "text/plain": [
       "<utils.Query at 0x7f5894984730>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "alpine-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryTokenizer:\n",
    "    \n",
    "    def __init__(self, tokenizer,col_orders=None):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.col_type_token_dict = {'text': '[unused11]', 'real': '[unused12]'}\n",
    "    \n",
    "    def tokenize(self, query: Query, col_orders=None):\n",
    "\n",
    "        question_tokens = ['[CLS]'] + self.tokenizer.tokenize(cc.convert(query.question.text))\n",
    "        header_tokens = []\n",
    "        \n",
    "        if col_orders is None:\n",
    "            col_orders = np.arange(len(query.table.header))\n",
    "        \n",
    "        header = [query.table.header[i] for i in col_orders]\n",
    "        \n",
    "        for col_name, col_type in header:\n",
    "            col_type_token = self.col_type_token_dict[col_type]\n",
    "            col_name = cc.convert(remove_brackets(col_name))\n",
    "            col_name_tokens = self.tokenizer.tokenize(col_name)\n",
    "            col_tokens = [col_type_token] + col_name_tokens\n",
    "            header_tokens.append(col_tokens)\n",
    "            \n",
    "        all_tokens = [question_tokens] + header_tokens\n",
    "        return self.pack(*all_tokens)\n",
    "    \n",
    "    def encode(self, query:Query, col_orders=None):\n",
    "        tokens, tokens_lens = self.tokenize(query, col_orders)\n",
    "#         token_ids = F.pad(torch.tensor(self.tokenizer.encode(tokens)[1:-1]), (0, max_len - len(tokens)))\n",
    "        token_ids = torch.as_tensor(self.tokenizer.encode(tokens)[1:-1])\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        attention_mask = [1] * len(token_ids)\n",
    "        header_indices = np.cumsum(tokens_lens)\n",
    "#         header_indices = F.pad(torch.tensor(header_indices[:-1]), (0, max_header_len - len(header_indices[:-1])))\n",
    "        header_indices = torch.as_tensor(header_indices[:-1])\n",
    "        return token_ids, attention_mask, segment_ids, header_indices\n",
    "    \n",
    "    def pack(self, *tokens_list):\n",
    "        packed_tokens_list = []\n",
    "        packed_tokens_lens = []\n",
    "        for tokens in tokens_list:\n",
    "            packed_tokens_list += tokens + ['[SEP]']\n",
    "            packed_tokens_lens.append(len(tokens) + 1)\n",
    "        return packed_tokens_list, packed_tokens_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "numeric-whale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokenizer = QueryTokenizer(tokenizer)\n",
    "sample_query = train_data[0]\n",
    "tokenizer.convert_ids_to_tokens(['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "modular-adobe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryTokenizer\n",
      "\n",
      "Input Question:\n",
      "下午三點以後從臺北到高雄的火車有哪些？\n",
      "\n",
      "Input Header:\n",
      "Type(text) | Train(text) | BreastFeed(text) | Route(text) | Package(text) | OverNightStn(text) | LineDir(text) | Line(text) | Dinning(text) | FoodSrv(text) | Cripple(text) | CarClass(text) | Bike(text) | ExtraTrain(text) | Everyday(text) | Note(text) | NoteEng(text) | Station(text) | Order(text) | DEPTime(real) | ARRTime(real) | ARRStation(text) | ARRDEPTime(real) | ARRARRTime(real)\n",
      "\n",
      "Output Tokens:\n",
      "[CLS] 下 午 三 點 以 後 從 臺 北 到 高 雄 的 火 車 有 哪 些 ？ [SEP] [unused11] type [SEP] [unused11] t ##rain [SEP] [unused11] br ##ea ##st ##fe ##ed [SEP] [unused11] ro ##ute [SEP] [unused11] pack ##age [SEP] [unused11] over ##night ##st ##n [SEP] [unused11] line ##di ##r [SEP] [unused11] line [SEP] [unused11] di ##nn ##ing [SEP] [unused11] food ##s ##r ##v [SEP] [unused11] cr ##ip ##ple [SEP] [unused11] car ##cl ##ass [SEP] [unused11] bi ##ke [SEP] [unused11] ex ##tra ##tra ##in [SEP] [unused11] ev ##ery ##day [SEP] [unused11] note [SEP] [unused11] note ##eng [SEP] [unused11] station [SEP] [unused11] order [SEP] [unused12] de ##pt ##ime [SEP] [unused12] ar ##rt ##ime [SEP] [unused11] ar ##rs ##tation [SEP] [unused12] ar ##rd ##ep ##time [SEP] [unused12] ar ##ra ##rr ##time [SEP]\n",
      "\n",
      "Output token_ids:\n",
      "tensor([  101,   678,  1286,   676,  7953,   809,  2527,  2537,  5637,  1266,\n",
      "         1168,  7770,  7413,  4638,  4125,  6722,  3300,  1525,   763,  8043,\n",
      "          102,    11,  9178,   102,    11,   162, 11944,   102,    11,  8575,\n",
      "        10073,  8415,  9568,  8303,   102,    11, 12910,  9710,   102,    11,\n",
      "        12736,  9103,   102,    11, 10047, 12734,  8415,  8171,   102,    11,\n",
      "         8323,  9172,  8180,   102,    11,  8323,   102,    11,  9796,  9502,\n",
      "         8221,   102,    11,  9579,  8118,  8180,  8225,   102,    11, 10951,\n",
      "         9032, 10383,   102,    11, 10875, 10753, 11904,   102,    11, 11055,\n",
      "         8537,   102,    11,  9577,  9808,  9808,  8277,   102,    11, 12311,\n",
      "        11041,  8758,   102,    11,  8698,   102,    11,  8698,  9995,   102,\n",
      "           11, 10459,   102,    11, 11156,   102,    12,  8363, 10789, 11218,\n",
      "          102,    12,  8673,  8716, 11218,   102,    11,  8673,  8640, 12386,\n",
      "          102,    12,  8673,  8642, 11613,  9785,   102,    12,  8673,  8332,\n",
      "        12579,  9785,   102])\n",
      "Output attention_mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Output segment_ids:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Output header_ids:\n",
      "tensor([ 21,  24,  28,  35,  39,  43,  49,  54,  57,  62,  68,  73,  78,  82,\n",
      "         88,  93,  96, 100, 103, 106, 111, 116, 121, 127])\n"
     ]
    }
   ],
   "source": [
    "def remove_brackets(s):\n",
    "    '''\n",
    "    Remove [] ()\n",
    "    '''\n",
    "    return re.sub(r'[\\(\\（].*[\\)\\）]', '', s)\n",
    "print('QueryTokenizer\\n')\n",
    "print('Input Question:\\n{}\\n'.format(sample_query.question))\n",
    "print('Input Header:\\n{}\\n'.format(sample_query.table.header))\n",
    "print('Output Tokens:\\n{}\\n'.format(' '.join(query_tokenizer.tokenize(sample_query)[0])))\n",
    "print('Output token_ids:\\n{}\\nOutput attention_mask:\\n{}\\nOutput segment_ids:\\n{}\\nOutput header_ids:\\n{}'\n",
    "      .format(*query_tokenizer.encode(sample_query)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "characteristic-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqlLabelEncoder:\n",
    "    \"\"\"\n",
    "    Convert SQL object into training labels.\n",
    "    \"\"\"\n",
    "    def encode(self, sql: SQL, num_cols):\n",
    "        cond_conn_op_label = sql.cond_conn_op\n",
    "        \n",
    "        sel_agg_label = np.ones(num_cols, dtype='int32') * len(SQL.agg_sql_dict)\n",
    "        for col_id, agg_op in zip(sql.sel, sql.agg):\n",
    "            if col_id < num_cols:\n",
    "                sel_agg_label[col_id] = agg_op\n",
    "            \n",
    "        cond_op_label = np.ones(num_cols, dtype='int32') * len(SQL.op_sql_dict)\n",
    "        for col_id, cond_op, _ in sql.conds:\n",
    "            if col_id < num_cols:\n",
    "                cond_op_label[col_id] = cond_op\n",
    "            \n",
    "        return cond_conn_op_label, sel_agg_label, cond_op_label\n",
    "    \n",
    "    def decode(self, cond_conn_op_label, sel_agg_label, cond_op_label):\n",
    "        cond_conn_op = int(cond_conn_op_label)\n",
    "        sel, agg, conds = [], [], []\n",
    "\n",
    "        for col_id, (agg_op, cond_op) in enumerate(zip(sel_agg_label, cond_op_label)):\n",
    "            if agg_op < len(SQL.agg_sql_dict):\n",
    "                sel.append(col_id)\n",
    "                agg.append(int(agg_op))\n",
    "            if cond_op < len(SQL.op_sql_dict):\n",
    "                conds.append([col_id, int(cond_op)])\n",
    "        return {\n",
    "            'sel': sel,\n",
    "            'agg': agg,\n",
    "            'cond_conn_op': cond_conn_op,\n",
    "            'conds': conds\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "seventh-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = SqlLabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "minus-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(sample_query.sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "narrative-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 tokenizer, \n",
    "                 label_encoder,\n",
    "                 is_train=True,\n",
    "                ):\n",
    "        \n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.is_train = is_train\n",
    "\n",
    "            \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        query = self.data[index]\n",
    "        question = query.question.text\n",
    "        table = query.table\n",
    "            \n",
    "        col_orders = np.arange(len(table.header))\n",
    "            \n",
    "        token_ids, attention_mask, segment_ids, header_ids = self.tokenizer.encode(query, col_orders)\n",
    "        header_ids = [hid for hid in header_ids]\n",
    "        \n",
    "        count = 0\n",
    "        for i in header_ids:\n",
    "            l = int(i)\n",
    "            if not l == 0:\n",
    "                count+=1\n",
    "                \n",
    "        header_mask = [1] * count\n",
    "        col_orders = col_orders[: len(header_ids)]\n",
    "         \n",
    "        \n",
    "        inputs = {\n",
    "            'input_token_ids': torch.as_tensor(token_ids),\n",
    "            'input_attention_mask': torch.as_tensor(attention_mask),\n",
    "            'input_segment_ids': torch.as_tensor(segment_ids),\n",
    "            'input_header_ids': torch.as_tensor(header_ids),\n",
    "            'input_header_mask': F.pad(torch.tensor(header_mask), (0, len(header_ids) - len(header_mask))),\n",
    "        }\n",
    "        if self.is_train:\n",
    "            true_sql = self.label_encoder.decode(*self.label_encoder.encode(query.sql, num_cols=len(query.table.header)))\n",
    "            sql = query.sql\n",
    "            \n",
    "            cond_conn_op, sel_agg, cond_op = self.label_encoder.encode(sql, num_cols=len(table.header))\n",
    "            \n",
    "            sel_agg = sel_agg[col_orders]\n",
    "            cond_op = cond_op[col_orders]\n",
    "\n",
    "            outputs = {\n",
    "                'output_sel_agg':  torch.as_tensor(sel_agg),\n",
    "                'output_cond_conn_op': torch.tensor(cond_conn_op),\n",
    "                'output_cond_op': torch.tensor(cond_op),\n",
    "            }\n",
    "            return inputs, outputs, true_sql\n",
    "        else:\n",
    "            return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "extra-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SQLDataset(train_data, query_tokenizer, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "japanese-optimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_token_ids': tensor([  101,   678,  1286,   676,  7953,   809,  2527,  2537,  5637,  1266,\n",
      "         1168,  7770,  7413,  4638,  4125,  6722,  3300,  1525,   763,  8043,\n",
      "          102,    11,  9178,   102,    11,   162, 11944,   102,    11,  8575,\n",
      "        10073,  8415,  9568,  8303,   102,    11, 12910,  9710,   102,    11,\n",
      "        12736,  9103,   102,    11, 10047, 12734,  8415,  8171,   102,    11,\n",
      "         8323,  9172,  8180,   102,    11,  8323,   102,    11,  9796,  9502,\n",
      "         8221,   102,    11,  9579,  8118,  8180,  8225,   102,    11, 10951,\n",
      "         9032, 10383,   102,    11, 10875, 10753, 11904,   102,    11, 11055,\n",
      "         8537,   102,    11,  9577,  9808,  9808,  8277,   102,    11, 12311,\n",
      "        11041,  8758,   102,    11,  8698,   102,    11,  8698,  9995,   102,\n",
      "           11, 10459,   102,    11, 11156,   102,    12,  8363, 10789, 11218,\n",
      "          102,    12,  8673,  8716, 11218,   102,    11,  8673,  8640, 12386,\n",
      "          102,    12,  8673,  8642, 11613,  9785,   102,    12,  8673,  8332,\n",
      "        12579,  9785,   102]), 'input_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'input_segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'input_header_ids': tensor([ 21,  24,  28,  35,  39,  43,  49,  54,  57,  62,  68,  73,  78,  82,\n",
      "         88,  93,  96, 100, 103, 106, 111, 116, 121, 127]), 'input_header_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}, {'output_sel_agg': tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6],\n",
      "       dtype=torch.int32), 'output_cond_conn_op': tensor(1), 'output_cond_op': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 2, 4, 4],\n",
      "       dtype=torch.int32)}, {'sel': [19], 'agg': [0], 'cond_conn_op': 1, 'conds': [[17, 2], [19, 0], [21, 2]]})\n",
      "({'input_token_ids': tensor([  101,   678,  1286,  1063,  7953,   809,  2527,  2537,  5637,   704,\n",
      "         1168,  7770,  7413,  4638,  4125,  6722,  3300,  1525,   763,  8043,\n",
      "          102,    11,  9178,   102,    11,   162, 11944,   102,    11,  8575,\n",
      "        10073,  8415,  9568,  8303,   102,    11, 12910,  9710,   102,    11,\n",
      "        12736,  9103,   102,    11, 10047, 12734,  8415,  8171,   102,    11,\n",
      "         8323,  9172,  8180,   102,    11,  8323,   102,    11,  9796,  9502,\n",
      "         8221,   102,    11,  9579,  8118,  8180,  8225,   102,    11, 10951,\n",
      "         9032, 10383,   102,    11, 10875, 10753, 11904,   102,    11, 11055,\n",
      "         8537,   102,    11,  9577,  9808,  9808,  8277,   102,    11, 12311,\n",
      "        11041,  8758,   102,    11,  8698,   102,    11,  8698,  9995,   102,\n",
      "           11, 10459,   102,    11, 11156,   102,    12,  8363, 10789, 11218,\n",
      "          102,    12,  8673,  8716, 11218,   102,    11,  8673,  8640, 12386,\n",
      "          102,    12,  8673,  8642, 11613,  9785,   102,    12,  8673,  8332,\n",
      "        12579,  9785,   102]), 'input_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'input_segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'input_header_ids': tensor([ 21,  24,  28,  35,  39,  43,  49,  54,  57,  62,  68,  73,  78,  82,\n",
      "         88,  93,  96, 100, 103, 106, 111, 116, 121, 127]), 'input_header_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}, {'output_sel_agg': tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6, 6, 6, 6],\n",
      "       dtype=torch.int32), 'output_cond_conn_op': tensor(1), 'output_cond_op': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 2, 4, 4],\n",
      "       dtype=torch.int32)}, {'sel': [19], 'agg': [0], 'cond_conn_op': 1, 'conds': [[17, 2], [19, 0], [21, 2]]})\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])\n",
    "print(train_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence,  pad_sequence\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_token_ids = []\n",
    "    input_attention_mask = []\n",
    "    input_segment_ids = []\n",
    "    input_header_ids = []\n",
    "    input_header_mask = []\n",
    "    output_sel_agg = []\n",
    "    output_cond_conn_op = []\n",
    "    output_cond_op = []\n",
    "    true_sqls = []\n",
    "    \n",
    "    for d in batch:\n",
    "        input_token_ids.append(d[0]['input_token_ids'])\n",
    "        input_attention_mask.append(d[0]['input_attention_mask'])\n",
    "        input_segment_ids.append(d[0]['input_segment_ids'])\n",
    "        input_header_ids.append(d[0]['input_header_ids'])\n",
    "        input_header_mask.append(d[0]['input_header_mask'])\n",
    "        output_sel_agg.append(d[1]['output_sel_agg'])\n",
    "        output_cond_conn_op.append(d[1]['output_cond_conn_op'])\n",
    "        output_cond_op.append(d[1]['output_cond_op'])\n",
    "        true_sqls.append(d[2])\n",
    "        \n",
    "    input_token_ids_len = torch.tensor([l.numel() for l in input_token_ids])\n",
    "    input_header_ids_len = torch.tensor([l.numel() for l in input_header_ids])\n",
    "    \n",
    "    padded_input_token_ids_batch = pad_sequence(input_token_ids, batch_first=True)\n",
    "    padded_input_attention_mask_batch = pad_sequence(input_attention_mask, batch_first=True)\n",
    "    padded_input_segment_ids_batch = pad_sequence(input_segment_ids, batch_first=True)\n",
    "    padded_input_header_ids_batch = pad_sequence(input_header_ids, batch_first=True)\n",
    "    padded_input_header_mask_batch = pad_sequence(input_header_mask, batch_first=True)\n",
    "    padded_output_sel_agg_batch = pad_sequence(output_sel_agg, batch_first=True, padding_value=-1)\n",
    "    padded_output_cond_conn_op_batch = torch.tensor(output_cond_conn_op).unsqueeze(-1)\n",
    "    padded_output_cond_op_batch = pad_sequence(output_cond_op, batch_first=True, padding_value=-1)\n",
    "\n",
    "    out1 = dict()\n",
    "    out2 = dict()\n",
    "    \n",
    "    out1['input_token_ids'] = padded_input_token_ids_batch\n",
    "    out1['input_attention_mask'] = padded_input_attention_mask_batch\n",
    "    out1['input_segment_ids'] = padded_input_segment_ids_batch\n",
    "    out1['input_header_ids'] = padded_input_header_ids_batch\n",
    "    out1['input_header_mask'] = padded_input_header_mask_batch\n",
    "    out2['output_sel_agg'] = padded_output_sel_agg_batch\n",
    "    out2['output_cond_conn_op'] = padded_output_cond_conn_op_batch\n",
    "    out2['output_cond_op'] = padded_output_cond_op_batch\n",
    "    \n",
    "    return (out1, out2), input_token_ids_len, input_header_ids_len, true_sqls\n",
    "\n",
    "def collate_fn2(batch):\n",
    "    input_token_ids = []\n",
    "    input_attention_mask = []\n",
    "    input_segment_ids = []\n",
    "    input_header_ids = []\n",
    "    input_header_mask = []\n",
    "    \n",
    "    for d in batch:\n",
    "        input_token_ids.append(d['input_token_ids'])\n",
    "        input_attention_mask.append(d['input_attention_mask'])\n",
    "        input_segment_ids.append(d['input_segment_ids'])\n",
    "        input_header_ids.append(d['input_header_ids'])\n",
    "        input_header_mask.append(d['input_header_mask'])\n",
    "        \n",
    "    input_token_ids_len = torch.tensor([l.numel() for l in input_token_ids])\n",
    "    input_header_ids_len = torch.tensor([l.numel() for l in input_header_ids])\n",
    "    \n",
    "    padded_input_token_ids_batch = pad_sequence(input_token_ids, batch_first=True)\n",
    "    padded_input_attention_mask_batch = pad_sequence(input_attention_mask, batch_first=True)\n",
    "    padded_input_segment_ids_batch = pad_sequence(input_segment_ids, batch_first=True)\n",
    "    padded_input_header_ids_batch = pad_sequence(input_header_ids, batch_first=True)\n",
    "    padded_input_header_mask_batch = pad_sequence(input_header_mask, batch_first=True)\n",
    "\n",
    "    out1 = dict()\n",
    "    \n",
    "    out1['input_token_ids'] = padded_input_token_ids_batch\n",
    "    out1['input_attention_mask'] = padded_input_attention_mask_batch\n",
    "    out1['input_segment_ids'] = padded_input_segment_ids_batch\n",
    "    out1['input_header_ids'] = padded_input_header_ids_batch\n",
    "    out1['input_header_mask'] = padded_input_header_mask_batch\n",
    "    \n",
    "    return (out1), input_token_ids_len, input_header_ids_len\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, num_workers=10)\n",
    "\n",
    "# a, b , c= train_loader.__iter__().__next__()\n",
    "# print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output sizes\n",
    "num_sel_agg = len(SQL.agg_sql_dict) + 1\n",
    "num_cond_op = len(SQL.op_sql_dict) + 1\n",
    "num_cond_conn_op = len(SQL.conn_sql_dict)\n",
    "\n",
    "print(num_sel_agg, num_cond_op, num_cond_conn_op)\n",
    "print(SQL.agg_sql_dict, SQL.op_sql_dict, SQL.conn_sql_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def seq_gather(x):\n",
    "    seq, idxs = x\n",
    "    idxs = torch.as_tensor(idxs).type(torch.int64)\n",
    "    offset = torch.arange(0, seq.size(0) * seq.size(1), seq.size(1)).to(device)\n",
    "    idxs = idxs + offset.unsqueeze(1)\n",
    "\n",
    "    seq = seq.reshape(-1, seq.shape[-1])[idxs]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _viterbi_decode(self, feats):\n",
    "        '''\n",
    "        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        # batch_transitions=self.transitions.expand(batch_size,self.tagset_size,self.tagset_size)\n",
    "\n",
    "        log_delta = torch.Tensor(batch_size, 1, self.tagset_size).fill_(-10000.).to(self.device)\n",
    "        log_delta[:, 0, self.start_label_id] = 0.\n",
    "        \n",
    "        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n",
    "        psi = torch.zeros((batch_size, T, self.tagset_size), dtype=torch.long)  # psi[0]=0000 useless\n",
    "        for t in range(1, T):\n",
    "            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n",
    "            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n",
    "            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n",
    "            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # trace back\n",
    "        path = torch.zeros((batch_size, T), dtype=torch.long)\n",
    "\n",
    "        # max p(z1:t,all_x|theta)\n",
    "        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n",
    "\n",
    "        for t in range(T-2, -1, -1):\n",
    "            # choose the state of z_t according the state choosed of z_t+1.\n",
    "            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n",
    "\n",
    "        return max_logLL_allz_allx, path\n",
    "    \n",
    "class MyBert(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, bert, device, freeze_bert=False):\n",
    "        super(MyBert, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        self.device = device\n",
    "        self.lambda1 = lambda x: x[:, 0]\n",
    "        self.out1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, num_cond_conn_op),\n",
    "#             torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.lambda2 = lambda x: torch.unsqueeze(x, -1)\n",
    "#         self.s1 = torch.nn.Sequential(\n",
    "#             torch.nn.Dropout(0.1),\n",
    "#             torch.nn.Linear(768, 512),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Dropout(0.1),\n",
    "#             torch.nn.Linear(512, 256),\n",
    "#             torch.nn.ReLU(),\n",
    "#         )\n",
    "        self.out2 = torch.nn.Linear(768, num_sel_agg)\n",
    "#         self.softmax1 = torch.nn.Softmax(dim=-1)\n",
    "#         self.s2 = torch.nn.Sequential(\n",
    "#             torch.nn.Dropout(0.1),\n",
    "#             torch.nn.Linear(775, 512),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Dropout(0.1),\n",
    "#             torch.nn.Linear(512, 256),\n",
    "#             torch.nn.ReLU(),\n",
    "#         )\n",
    "        self.out3 = torch.nn.Linear(775, num_cond_op)\n",
    "#         self.softmax2 = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, query: tuple, input_token_ids_len, input_header_ids_len):\n",
    "        bert_out = self.bert(query['input_token_ids'].to(self.device), query['input_attention_mask'].to(self.device), token_type_ids=query['input_segment_ids'].to(self.device))\n",
    "        logit1 = self.out1(self.lambda1(bert_out[0]))\n",
    "        logit2 = seq_gather((bert_out[0], query['input_header_ids'].to(self.device)))\n",
    "        logit3 = self.lambda2(query['input_header_mask'].to(self.device))\n",
    "        logit4 = logit2 * logit3\n",
    "#         logit5 = logit4.matmul(self.out2.weight.t() * self.mask1)\n",
    "#         if self.out2.bias is not None:\n",
    "#             logit5 += torch.jit._unwrap_optional(self.out2.bias)\n",
    "#         logit5 = self.softmax1(logit5)\n",
    "        logit5 = self.out2(logit4)\n",
    "        logit6 = torch.cat((logit5, logit4), 2)\n",
    "#         logit6 = logit6.matmul(self.out3.weight.t() * self.mask2)\n",
    "#         if self.out3.bias is not None:\n",
    "#             logit6 += torch.jit._unwrap_optional(self.out3.bias)\n",
    "        logit7 = self.out3(logit6)\n",
    "        return logit1, logit5, logit7\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, header_lens, label_encoder):\n",
    "    \"\"\"\n",
    "    Generate sqls from model outputs\n",
    "    \"\"\"\n",
    "    preds_cond_conn_op = torch.softmax(preds_cond_conn_op, axis=-1)\n",
    "    preds_sel_agg = torch.softmax(preds_sel_agg, axis=-1)\n",
    "    preds_cond_op = torch.softmax(preds_cond_op, axis=-1)\n",
    "    preds_cond_conn_op = torch.argmax(preds_cond_conn_op, axis=-1)\n",
    "    preds_cond_op = torch.argmax(preds_cond_op, axis=-1)\n",
    "\n",
    "    sqls = []\n",
    "    \n",
    "    for cond_conn_op, sel_agg, cond_op, header_len in zip(preds_cond_conn_op, \n",
    "                                                          preds_sel_agg, \n",
    "                                                          preds_cond_op, \n",
    "                                                          header_lens):\n",
    "        sel_agg = sel_agg[:header_len]\n",
    "        # force to select at least one column for agg\n",
    "        t = sel_agg[:, :-1].max()\n",
    "        one = torch.ones_like(sel_agg)\n",
    "        sel_agg =torch.where(sel_agg == t, one, sel_agg)\n",
    "        sel_agg = torch.argmax(sel_agg, axis=-1)\n",
    "        sql = label_encoder.decode(cond_conn_op, sel_agg, cond_op)\n",
    "        sql['conds'] = [cond for cond in sql['conds'] if cond[0] < header_len]\n",
    "        sel = []\n",
    "        agg = []\n",
    "        for col_id, agg_op in zip(sql['sel'], sql['agg']):\n",
    "            if col_id < header_len:\n",
    "                sel.append(col_id)\n",
    "                agg.append(agg_op)\n",
    "                \n",
    "        sql['sel'] = sel\n",
    "        sql['agg'] = agg\n",
    "        sqls.append(sql)\n",
    "    return sqls\n",
    "\n",
    "from torch.utils import tensorboard\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "# from torch_optimizer import RAdam\n",
    "\n",
    "epoch = 100\n",
    "total_steps = len(train_loader) * epoch\n",
    "\n",
    "# optim = RAdam(model.parameters(), lr=5e-5, eps=1e-07)\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(optim,\n",
    "#                                             num_warmup_steps = 0, \n",
    "#                                             num_training_steps = total_steps)\n",
    "\n",
    "objtv1 = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "objtv2 = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "objtv3 = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "step = 0\n",
    "\n",
    "exp_path = r'./model_save4'\n",
    "# Create experiment folder.\n",
    "exp_path = os.path.join(exp_path, r'test')\n",
    "\n",
    "if not os.path.exists(exp_path):\n",
    "    os.makedirs(exp_path)\n",
    "\n",
    "# Create logger and log folder.\n",
    "writer = tensorboard.SummaryWriter(\n",
    "    os.path.join('tens')\n",
    ")\n",
    "\n",
    "# Log average loss.\n",
    "total_loss = 0.0\n",
    "pre_total_loss = 0.0\n",
    "total_cond_op_loss = 0.0\n",
    "pre_cond_op_loss = 0.0\n",
    "total_sel_agg_loss = 0.0\n",
    "pre_sel_agg_loss = 0.0\n",
    "total_cond_conn_op_loss = 0.0\n",
    "pre_cond_conn_op_loss = 0.0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('pooler'):\n",
    "        param.requires_grad_(False)\n",
    "    else:\n",
    "        continue\n",
    "                    \n",
    "bert_model = MyBert(model, device).to(device)\n",
    "\n",
    "import torch.optim as optimizer\n",
    "from ranger import Ranger\n",
    "optim = Ranger(bert_model.parameters(), lr=2e-5, eps=1e-07)\n",
    "scheduler = optimizer.lr_scheduler.CosineAnnealingWarmRestarts(optim,T_0=5,T_mult=1)\n",
    "# optim = RAdam(bert_model.parameters(), lr=5e-5, eps=1e-07)\n",
    "print(bert_model)\n",
    "count0 = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "count0 = sum(p.numel() for n, p in bert_model.named_parameters() if n.startswith('out1'))\n",
    "count1 = sum(p.numel() for n, p in bert_model.named_parameters() if n.startswith('out2'))\n",
    "count2 = sum(p.numel() for n, p in bert_model.named_parameters() if n.startswith('out3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.load_state_dict(torch.load('./model_save2/test9/model.pt'))\n",
    "for cur_epoch in range(epoch):\n",
    "    tqdm_dldr = tqdm(\n",
    "        train_loader,\n",
    "        desc=f'epoch: {cur_epoch}, loss: {pre_total_loss:.6f} , total_cond_op_loss: {pre_cond_op_loss:.6f}, total_sel_agg_loss: {pre_sel_agg_loss:.6f}, total_cond_conn_op_loss: {pre_cond_conn_op_loss:.6f}'\n",
    "    )\n",
    "    bert_model.train()\n",
    "    for i, batch in enumerate(tqdm_dldr):\n",
    "        b, len1, len2, _= batch\n",
    "        x, y = b\n",
    "        \n",
    "        # Clean up gradient.\n",
    "        optim.zero_grad()\n",
    "        # Forward pass.\n",
    "        logits = bert_model(\n",
    "            x,\n",
    "            len1,\n",
    "            len2,\n",
    "        )\n",
    "        \n",
    "        out_cond_conn_op, out_sel_agg, out_cond_op = logits\n",
    "\n",
    "        y['output_cond_op'] = y['output_cond_op'].to(device, dtype=torch.int64)\n",
    "        y['output_sel_agg'] = y['output_sel_agg'].to(device, dtype=torch.int64)\n",
    "        y['output_cond_conn_op'] = y['output_cond_conn_op'].to(device, dtype=torch.int64)\n",
    "        \n",
    "        # Calculate loss.\n",
    "        cond_op_loss = objtv1(\n",
    "            out_cond_op.reshape(out_cond_op.shape[0] * out_cond_op.shape[1], -1),\n",
    "            y['output_cond_op'].reshape(-1),\n",
    "        )\n",
    "        \n",
    "        sel_agg_loss = objtv2(\n",
    "            out_sel_agg.reshape(out_sel_agg.shape[0] * out_sel_agg.shape[1], -1),\n",
    "            y['output_sel_agg'].reshape(-1),\n",
    "        )\n",
    "\n",
    "        cond_conn_op_loss = objtv3(\n",
    "            out_cond_conn_op,\n",
    "            y['output_cond_conn_op'].reshape(-1),\n",
    "        )\n",
    "\n",
    "        loss = cond_op_loss + sel_agg_loss + cond_conn_op_loss\n",
    "        \n",
    "        # Accumulate loss.\n",
    "        total_loss += loss.item()\n",
    "        total_cond_op_loss += cond_op_loss.item()\n",
    "        total_sel_agg_loss += sel_agg_loss.item()\n",
    "        total_cond_conn_op_loss += cond_conn_op_loss.item()\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=bert_model.parameters(),\n",
    "            max_norm=1.0,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Gradient descent.\n",
    "        scheduler.step(cur_epoch + i / 5)\n",
    "        optim.step()\n",
    "\n",
    "        # Log average loss on CLI.\n",
    "        tqdm_dldr.set_description(\n",
    "            desc=f'epoch: {cur_epoch}, loss: {pre_total_loss:.6f} , total_cond_op_loss: {pre_cond_op_loss:.6f}, total_sel_agg_loss: {pre_sel_agg_loss:.6f}, total_cond_conn_op_loss: {pre_cond_conn_op_loss:.6f}'\n",
    "        )\n",
    "        step += 1\n",
    "#        # Log average loss on tensorboard.\n",
    "        if step%500 == 0:\n",
    "            writer.add_scalar('Train/Loss', total_loss/(i+1), step)\n",
    "            writer.flush()\n",
    "        pre_total_loss = total_loss / (i+1)\n",
    "        pre_cond_op_loss = total_cond_op_loss / (i+1)\n",
    "        pre_sel_agg_loss = total_sel_agg_loss / (i+1)\n",
    "        pre_cond_conn_op_loss = total_cond_conn_op_loss / (i+1)\n",
    "\n",
    "    # Clean up average loss.\n",
    "    total_loss = 0.0\n",
    "    total_cond_op_loss = 0.0\n",
    "    total_sel_agg_loss = 0.0\n",
    "    total_cond_conn_op_loss = 0.0\n",
    "    tqdm_dldr.set_description(\n",
    "        desc=f'epoch: {cur_epoch}, loss: {pre_total_loss:.6f} , total_cond_op_loss: {pre_cond_op_loss:.6f}, total_sel_agg_loss: {pre_sel_agg_loss:.6f}, total_cond_conn_op_loss: {pre_cond_conn_op_loss:.6f}'\n",
    "    )\n",
    "    \n",
    "\n",
    "# Save last checkpoint.\n",
    "torch.save(\n",
    "    bert_model.state_dict(),\n",
    "    os.path.join(exp_path, f'model.pt'),\n",
    ")\n",
    "\n",
    "# Close logger.\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = MyBert(model, device).to(device)\n",
    "# bert_model.load_state_dict(torch.load('./model_save2/test9/model.pt'))\n",
    "test_table_file = './table.json'\n",
    "test_data_file = './4.json'\n",
    "\n",
    "test_tables = read_tables(test_table_file)\n",
    "test_data, test_max_len, test_max_header_len = read_data(test_data_file, test_tables)\n",
    "\n",
    "test_set = SQLDataset(test_data, query_tokenizer, label_encoder, is_train=False)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_set, batch_size=batch_size, collate_fn=collate_fn2, shuffle=False, num_workers=10)\n",
    "\n",
    "bert_model.eval()\n",
    "\n",
    "\n",
    "pred_sqls = []\n",
    "with torch.no_grad():\n",
    "    for b, len1, len2 in test_loader:\n",
    "        header_lens = torch.sum(b['input_header_mask'], axis=-1)\n",
    "        preds_cond_conn_op, preds_sel_agg, preds_cond_op = bert_model(b, len1, len2)\n",
    "        sqls = outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, \n",
    "                                   header_lens, label_encoder)\n",
    "        pred_sqls += sqls\n",
    "# print(pred_sqls)\n",
    "task1_output_file = '5.json'\n",
    "with open(task1_output_file, 'w') as f:\n",
    "    for sql in pred_sqls:\n",
    "        json_str = json.dumps(sql, ensure_ascii=False)\n",
    "        f.write(json_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worst-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import cn2an\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils import read_data, read_tables, SQL, Query, Question, Table, RAdam\n",
    "from keras_bert import get_checkpoint_paths, load_vocabulary, Tokenizer, load_trained_model_from_checkpoint\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Lambda, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from opencc import OpenCC\n",
    "cc = OpenCC('s2twp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "foster-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_file = './table.json'\n",
    "test_data_file = './4.json'\n",
    "\n",
    "train_table_file = './table.json'\n",
    "train_data_file = './t.json'\n",
    "\n",
    "# Download pretrained BERT model from https://github.com/ymcui/Chinese-BERT-wwm\n",
    "bert_model_path = './model/publish'\n",
    "\n",
    "paths = get_checkpoint_paths(bert_model_path)\n",
    "\n",
    "task1_file = './5.json'\n",
    "\n",
    "test_tables = read_tables(test_table_file)\n",
    "test_data = read_data(test_data_file, test_tables)\n",
    "\n",
    "train_tables = read_tables(train_table_file)\n",
    "train_data = read_data(train_data_file, train_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expired-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def cn_to_an(string):\n",
    "    try:\n",
    "        return str(cn2an.cn2an(string, 'normal'))\n",
    "    except ValueError:\n",
    "        return string\n",
    "\n",
    "def an_to_cn(string):\n",
    "    try:\n",
    "        return str(cn2an.an2cn(string))\n",
    "    except ValueError:\n",
    "        return string\n",
    "\n",
    "def str_to_num(string):\n",
    "    try:\n",
    "        float_val = float(cn_to_an(string))\n",
    "        if int(float_val) == float_val:   \n",
    "            return str(int(float_val))\n",
    "        else:\n",
    "            return str(float_val)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def str_to_year(string):\n",
    "    year = string.replace('年', '')\n",
    "    year = cn_to_an(year)\n",
    "    if is_float(year) and float(year) < 1900:\n",
    "        year = int(year) + 2000\n",
    "        return str(year)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def load_json(json_file):\n",
    "    result = []\n",
    "    if json_file:\n",
    "        with open(json_file) as file:\n",
    "            for line in file:\n",
    "                result.append(json.loads(line))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "secret-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionCondPair:\n",
    "    def __init__(self, query_id, question, cond_text, cond_sql, label):\n",
    "        self.query_id = query_id\n",
    "        self.question = question\n",
    "        self.cond_text = cond_text\n",
    "        self.cond_sql = cond_sql\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = ''\n",
    "        repr_str += 'query_id: {}\\n'.format(self.query_id)\n",
    "        repr_str += 'question: {}\\n'.format(self.question)\n",
    "        repr_str += 'cond_text: {}\\n'.format(self.cond_text)\n",
    "        repr_str += 'cond_sql: {}\\n'.format(self.cond_sql)\n",
    "        repr_str += 'label: {}\\n'.format(self.label)\n",
    "        return repr_str\n",
    "\n",
    "    \n",
    "class NegativeSampler:\n",
    "    \"\"\"\n",
    "    从 question - cond pairs 中采样\n",
    "    \"\"\"\n",
    "    def __init__(self, neg_sample_ratio=10):\n",
    "        self.neg_sample_ratio = neg_sample_ratio\n",
    "    \n",
    "    def sample(self, data):\n",
    "        positive_data = [d for d in data if d.label == 1]\n",
    "        negative_data = [d for d in data if d.label == 0]\n",
    "        negative_sample = random.sample(negative_data, \n",
    "                                        len(positive_data) * self.neg_sample_ratio)\n",
    "        return positive_data + negative_sample\n",
    "\n",
    "    \n",
    "class FullSampler:\n",
    "    \"\"\"\n",
    "    不抽样，返回所有的 pairs\n",
    "    \n",
    "    \"\"\"\n",
    "    def sample(self, data):\n",
    "        return data\n",
    "\n",
    "class CandidateCondsExtractor:\n",
    "    \"\"\"\n",
    "    params:\n",
    "        - share_candidates: 在同 table 同 column 中共享 real 型 candidates\n",
    "    \"\"\"\n",
    "    CN_NUM = '〇一二三四五六七八九零壹贰叁肆伍陆柒捌玖貮两'\n",
    "    CN_UNIT = '十拾百佰千仟万萬亿億兆点'\n",
    "    \n",
    "    CN_NUM = cc.convert(CN_NUM)\n",
    "    CN_UNIT = cc.convert(CN_UNIT)\n",
    "    \n",
    "    def __init__(self, share_candidates=True):\n",
    "        self.share_candidates = share_candidates\n",
    "        self._cached = False\n",
    "    \n",
    "    def build_candidate_cache(self, queries):\n",
    "        self.cache = defaultdict(set)\n",
    "        for query_id, query in tqdm(enumerate(queries), total=len(queries)):\n",
    "            value_in_question = self.extract_values_from_text(query.question.text)\n",
    "#             print(query.question.text, value_in_question)\n",
    "            \n",
    "            for col_id, (col_name, col_type) in enumerate(query.table.header):\n",
    "                value_in_column = self.extract_values_from_column(query, col_id)\n",
    "                if col_type == 'text':\n",
    "                    cond_values = value_in_column\n",
    "                elif col_type == 'real':\n",
    "                    if len(value_in_column) == 1: \n",
    "                        cond_values = value_in_column + value_in_question\n",
    "                    else:\n",
    "                        cond_values = value_in_question\n",
    "                cache_key = self.get_cache_key(query_id, query, col_id)\n",
    "                self.cache[cache_key].update(cond_values)\n",
    "        self._cached = True\n",
    "    \n",
    "    def get_cache_key(self, query_id, query, col_id):\n",
    "        if self.share_candidates:\n",
    "            return (query.table.id, col_id)\n",
    "        else:\n",
    "            return (query_id, query.table.id, col_id)\n",
    "        \n",
    "    def extract_year_from_text(self, text):\n",
    "        values = []\n",
    "        num_year_texts = re.findall(r'[0-9][0-9]年', text)\n",
    "        values += ['20{}'.format(text[:-1]) for text in num_year_texts]\n",
    "        cn_year_texts = re.findall(r'[{}][{}]年'.format(self.CN_NUM, self.CN_NUM), text)\n",
    "        cn_year_values = [str_to_year(text) for text in cn_year_texts]\n",
    "        values += [value for value in cn_year_values if value is not None]\n",
    "        return values\n",
    "    \n",
    "    def extract_num_from_text(self, text):\n",
    "        values = []\n",
    "        num_values = re.findall(r'[-+]?[0-9]*\\.?[0-9]+', text)\n",
    "        values += num_values\n",
    "        \n",
    "        cn_num_unit = self.CN_NUM + self.CN_UNIT\n",
    "        cn_num_texts = re.findall(r'[{}]*\\.?[{}]+'.format(cn_num_unit, cn_num_unit), text)\n",
    "        if len(cn_num_texts[0]) > 1:\n",
    "            cn_num_values = [str_to_num(text[:-1]) for text in cn_num_texts]\n",
    "        else:\n",
    "            cn_num_values = [str_to_num(text) for text in cn_num_texts]\n",
    "        \n",
    "        cf = re.search(r'[{}+]午'.format('下'), text)\n",
    "        ccf = re.search(r'[{}+]上'.format('晚'), text)\n",
    "        cccf = re.search(r'[{}+]晚'.format('傍'), text)\n",
    "        hf = re.search(r'[{}+]半'.format('點'), text)\n",
    "\n",
    "        values += [value for value in cn_num_values if value is not None]\n",
    "        \n",
    "        cn_num_mix = re.findall(r'[0-9]*\\.?[{}]+'.format(self.CN_UNIT), text)\n",
    "        for word in cn_num_mix:\n",
    "            num = re.findall(r'[-+]?[0-9]*\\.?[0-9]+', word)\n",
    "            for n in num:\n",
    "                word = word.replace(n, an_to_cn(n))\n",
    "            str_num = str_to_num(word)\n",
    "            if str_num is not None and len(values) == 0:\n",
    "                values.append(str_num)\n",
    "                \n",
    "        if cf != None or ccf != None or cccf != None:\n",
    "            values = [str(int(values[0]) + 12)]\n",
    "        if values[0] == '24':\n",
    "            values[0] = '0'\n",
    "         \n",
    "        if hf is not None:\n",
    "            v = [values[0] + ':30:00']\n",
    "        else:\n",
    "            v = [values[0] + ':00:00']\n",
    "        \n",
    "        if len(values[0]) == 1:\n",
    "            v = ['0' + v[0]]\n",
    "        \n",
    "        print(v)\n",
    "    \n",
    "        return v\n",
    "    \n",
    "    def extract_values_from_text(self, text):\n",
    "        values = []\n",
    "        values += self.extract_year_from_text(text)\n",
    "        values += self.extract_num_from_text(text)\n",
    "        return list(set(values))\n",
    "   \n",
    "    def extract_values_from_column(self, query, col_ids):\n",
    "        question = query.question.text\n",
    "        question_chars = set(query.question.text)\n",
    "        unique_col_values = set(query.table.df.iloc[:, col_ids].astype(str))\n",
    "        select_col_values = [v for v in unique_col_values \n",
    "                             if (question_chars & set(v))]\n",
    "        return select_col_values\n",
    "    \n",
    "    \n",
    "class QuestionCondPairsDataset:\n",
    "    \"\"\"\n",
    "    question - cond pairs 数据集\n",
    "    \"\"\"\n",
    "    OP_PATTERN = {\n",
    "        'real':\n",
    "        [\n",
    "            {'cond_op_idx': 0, 'pattern': '{col_name}大于{value}'},\n",
    "            {'cond_op_idx': 1, 'pattern': '{col_name}小于{value}'},\n",
    "            {'cond_op_idx': 2, 'pattern': '{col_name}是{value}'}\n",
    "        ],\n",
    "        'text':\n",
    "        [\n",
    "            {'cond_op_idx': 2, 'pattern': '{col_name}是{value}'}\n",
    "        ]\n",
    "    }    \n",
    "    \n",
    "    def __init__(self, queries, candidate_extractor, has_label=True, model_1_outputs=None):\n",
    "        self.candidate_extractor = candidate_extractor\n",
    "        self.has_label = has_label\n",
    "        self.model_1_outputs = model_1_outputs\n",
    "        self.data = self.build_dataset(queries[0])\n",
    "        \n",
    "    def build_dataset(self, queries):\n",
    "        if not self.candidate_extractor._cached:\n",
    "            self.candidate_extractor.build_candidate_cache(queries)\n",
    "            \n",
    "        pair_data = []\n",
    "        for query_id, query in enumerate(queries):\n",
    "            select_col_id = self.get_select_col_id(query_id, query)\n",
    "            for col_id, (col_name, col_type) in enumerate(query.table.header):\n",
    "                if col_id not in select_col_id:\n",
    "                    continue\n",
    "                    \n",
    "                cache_key = self.candidate_extractor.get_cache_key(query_id, query, col_id)\n",
    "                values = self.candidate_extractor.cache.get(cache_key, [])\n",
    "                pattern = self.OP_PATTERN.get(col_type, [])\n",
    "                pairs = self.generate_pairs(query_id, query, col_id, col_name, \n",
    "                                               values, pattern)\n",
    "                pair_data += pairs\n",
    "        return pair_data\n",
    "    \n",
    "    def get_select_col_id(self, query_id, query):\n",
    "        if self.model_1_outputs:\n",
    "            select_col_id = [cond_col for cond_col, *_ in self.model_1_outputs[query_id]['conds']]\n",
    "        elif self.has_label:\n",
    "            select_col_id = [cond_col for cond_col, *_ in query.sql.conds]\n",
    "        else:\n",
    "            select_col_id = list(range(len(query.table.header)))\n",
    "        return select_col_id\n",
    "            \n",
    "    def generate_pairs(self, query_id, query, col_id, col_name, values, op_patterns):\n",
    "        pairs = []\n",
    "        for value in values:\n",
    "            for op_pattern in op_patterns:\n",
    "                cond = op_pattern['pattern'].format(col_name=col_name, value=value)\n",
    "                cond_sql = (col_id, op_pattern['cond_op_idx'], value)\n",
    "                real_sql = {}\n",
    "                if self.has_label:\n",
    "                    real_sql = {tuple(c) for c in query.sql.conds}\n",
    "                label = 1 if cond_sql in real_sql else 0\n",
    "                pair = QuestionCondPair(query_id, query.question.text,\n",
    "                                        cond, cond_sql, label)\n",
    "                pairs.append(pair)\n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inner-clearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-6739f8b7c584>:59: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for query_id, query in tqdm(enumerate(queries), total=len(queries)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06561fb70e641a5949c524f2071d44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15:00:00']\n",
      "['18:00:00']\n",
      "['15:00:00']\n",
      "['17:00:00']\n",
      "['17:00:00']\n",
      "['08:00:00']\n",
      "['08:00:00']\n",
      "['08:00:00']\n",
      "['06:00:00']\n",
      "['11:00:00']\n",
      "['10:00:00']\n",
      "['08:00:00']\n",
      "['07:00:00']\n",
      "['18:00:00']\n",
      "['18:00:00']\n",
      "['20:00:00']\n",
      "['11:00:00']\n",
      "['20:00:00']\n",
      "['06:00:00']\n",
      "['07:00:00']\n",
      "['21:00:00']\n",
      "['21:00:00']\n",
      "['00:00:00']\n",
      "['19:00:00']\n",
      "['06:00:00']\n",
      "['12:00:00']\n",
      "['13:00:00']\n",
      "['13:00:00']\n",
      "['12:00:00']\n",
      "['03:00:00']\n",
      "['03:00:00']\n",
      "['08:00:00']\n",
      "['08:00:00']\n",
      "['09:00:00']\n",
      "['09:00:00']\n",
      "['08:00:00']\n",
      "['08:00:00']\n",
      "['09:00:00']\n",
      "['09:00:00']\n",
      "['03:00:00']\n",
      "['03:00:00']\n",
      "['04:00:00']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ed7a4c85024e7b925318b54def2937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15:00:00']\n",
      "['08:00:00']\n",
      "['03:00:00']\n"
     ]
    }
   ],
   "source": [
    "task1_result = load_json(task1_file)\n",
    "tr_qc_pairs = QuestionCondPairsDataset(train_data, \n",
    "                                       candidate_extractor=CandidateCondsExtractor(share_candidates=False))\n",
    "\n",
    "te_qc_pairs = QuestionCondPairsDataset(test_data, \n",
    "                                       candidate_extractor=CandidateCondsExtractor(share_candidates=True),\n",
    "                                       has_label=False,\n",
    "                                       model_1_outputs=task1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "treated-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]')\n",
    "            else:\n",
    "                R.append('[UNK]')\n",
    "        return R\n",
    "\n",
    "            \n",
    "def construct_model(paths, use_multi_gpus=False):\n",
    "    token_dict = load_vocabulary(paths.vocab)\n",
    "    tokenizer = SimpleTokenizer(token_dict)\n",
    "\n",
    "    bert_model = load_trained_model_from_checkpoint(\n",
    "        paths.config, paths.checkpoint, seq_len=None)\n",
    "    for l in bert_model.layers:\n",
    "        l.trainable = True\n",
    "\n",
    "    x1_in = Input(shape=(None,), name='input_x1', dtype='int32')\n",
    "    x2_in = Input(shape=(None,), name='input_x2')\n",
    "    x = bert_model([x1_in, x2_in])\n",
    "    x_cls = Lambda(lambda x: x[:, 0])(x)\n",
    "    y_pred = Dense(1, activation='sigmoid', name='output_similarity')(x_cls)\n",
    "\n",
    "    model = Model([x1_in, x2_in], y_pred)\n",
    "    if use_multi_gpus:\n",
    "        print('using multi-gpus')\n",
    "        model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss={'output_similarity': 'binary_crossentropy'},\n",
    "                  optimizer=Adam(lr=1e-5),\n",
    "                  metrics={'output_similarity': 'accuracy'})\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brave-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = construct_model(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adopted-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionCondPairsDataseq(Sequence):\n",
    "    def __init__(self, dataset, tokenizer, is_train=True, max_len=120, \n",
    "                 sampler=None, shuffle=False, batch_size=32):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_train = is_train\n",
    "        self.max_len = max_len\n",
    "        self.sampler = sampler\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()       \n",
    "    \n",
    "    def _pad_sequences(self, seqs, max_len=None):\n",
    "        return pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    def __getitem__(self, batch_id):\n",
    "        batch_data_indices = \\\n",
    "            self.global_indices[batch_id * self.batch_size: (batch_id + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_data_indices]\n",
    "\n",
    "        X1, X2 = [], []\n",
    "        Y = []\n",
    "        \n",
    "        for data in batch_data:\n",
    "            x1, x2 = self.tokenizer.encode(first=data.question.lower(), \n",
    "                                           second=data.cond_text.lower())\n",
    "            X1.append(x1)\n",
    "            X2.append(x2)\n",
    "            if self.is_train:\n",
    "                Y.append([data.label])\n",
    "    \n",
    "        X1 = self._pad_sequences(X1, max_len=self.max_len)\n",
    "        X2 = self._pad_sequences(X2, max_len=self.max_len)\n",
    "        inputs = {'input_x1': X1, 'input_x2': X2}\n",
    "        if self.is_train:\n",
    "            Y = self._pad_sequences(Y, max_len=1)\n",
    "            outputs = {'output_similarity': Y}\n",
    "            return inputs, outputs\n",
    "        else:\n",
    "            return inputs\n",
    "                    \n",
    "    def on_epoch_end(self):\n",
    "        self.data = self.sampler.sample(self.dataset)\n",
    "        self.global_indices = np.arange(len(self.data))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.global_indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "patent-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_qc_pairs_seq = QuestionCondPairsDataseq(tr_qc_pairs, tokenizer, \n",
    "                                           sampler=NegativeSampler(), shuffle=True)\n",
    "\n",
    "te_qc_pairs_seq = QuestionCondPairsDataseq(te_qc_pairs, tokenizer, \n",
    "                                           sampler=FullSampler(), shuffle=False, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "useful-maldives",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonchang/anaconda3/envs/bert/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41/41 [==============================] - 23s 357ms/step - loss: 0.2357 - accuracy: 0.9074\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 15s 355ms/step - loss: 0.0977 - accuracy: 0.9507\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 15s 370ms/step - loss: 0.0593 - accuracy: 0.9719\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 15s 363ms/step - loss: 0.0116 - accuracy: 0.9985\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 15s 357ms/step - loss: 0.0080 - accuracy: 0.9976\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 15s 357ms/step - loss: 0.0105 - accuracy: 0.9962\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 15s 357ms/step - loss: 0.0104 - accuracy: 0.9959\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 15s 358ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 15s 359ms/step - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 15s 359ms/step - loss: 0.0158 - accuracy: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f59841269d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('task2_best_model.h5')\n",
    "model.fit_generator(tr_qc_pairs_seq, epochs=10, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "matched-peter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasonchang/anaconda3/envs/bert/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 3s 54ms/step\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow import keras\n",
    "# model.load_weights('task2_best_model.h5')\n",
    "te_result = model.predict_generator(te_qc_pairs_seq, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unlike-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_result(qc_pairs, result, threshold):\n",
    "    select_result = defaultdict(set)\n",
    "    for pair, score in zip(qc_pairs, result):\n",
    "        print(pair, score)\n",
    "        if score > threshold:\n",
    "            select_result[pair.query_id].update([pair.cond_sql])\n",
    "    return dict(select_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "occupied-tamil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是竹北\n",
      "cond_sql: (17, 2, '竹北')\n",
      "label: 0\n",
      " [2.775646e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是嘉北\n",
      "cond_sql: (17, 2, '嘉北')\n",
      "label: 0\n",
      " [3.9527418e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是知本\n",
      "cond_sql: (17, 2, '知本')\n",
      "label: 0\n",
      " [1.2862415e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是三姓橋\n",
      "cond_sql: (17, 2, '三姓橋')\n",
      "label: 0\n",
      " [5.2638245e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是北新竹\n",
      "cond_sql: (17, 2, '北新竹')\n",
      "label: 0\n",
      " [1.749445e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是高雄\n",
      "cond_sql: (17, 2, '高雄')\n",
      "label: 0\n",
      " [4.1190706e-05]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是彰化\n",
      "cond_sql: (17, 2, '彰化')\n",
      "label: 0\n",
      " [1.5574585e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是中洲\n",
      "cond_sql: (17, 2, '中洲')\n",
      "label: 0\n",
      " [1.3923457e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是臺東\n",
      "cond_sql: (17, 2, '臺東')\n",
      "label: 0\n",
      " [6.365286e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是八堵\n",
      "cond_sql: (17, 2, '八堵')\n",
      "label: 0\n",
      " [1.9754226e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是北埔\n",
      "cond_sql: (17, 2, '北埔')\n",
      "label: 0\n",
      " [1.8109164e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是三義\n",
      "cond_sql: (17, 2, '三義')\n",
      "label: 0\n",
      " [9.611999e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是後龍\n",
      "cond_sql: (17, 2, '後龍')\n",
      "label: 0\n",
      " [1.3704471e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是善化\n",
      "cond_sql: (17, 2, '善化')\n",
      "label: 0\n",
      " [9.232803e-08]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是三民\n",
      "cond_sql: (17, 2, '三民')\n",
      "label: 0\n",
      " [3.3293807e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是中里\n",
      "cond_sql: (17, 2, '中里')\n",
      "label: 0\n",
      " [1.9145322e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是臺北\n",
      "cond_sql: (17, 2, '臺北')\n",
      "label: 0\n",
      " [0.99903715]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是北湖\n",
      "cond_sql: (17, 2, '北湖')\n",
      "label: 0\n",
      " [2.924992e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是臺中\n",
      "cond_sql: (17, 2, '臺中')\n",
      "label: 0\n",
      " [6.24276e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是後庄\n",
      "cond_sql: (17, 2, '後庄')\n",
      "label: 0\n",
      " [1.5718098e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是後壁\n",
      "cond_sql: (17, 2, '後壁')\n",
      "label: 0\n",
      " [1.2464837e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是桃園\n",
      "cond_sql: (17, 2, '桃園')\n",
      "label: 0\n",
      " [1.2263878e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是竹中\n",
      "cond_sql: (17, 2, '竹中')\n",
      "label: 0\n",
      " [2.356748e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是民雄\n",
      "cond_sql: (17, 2, '民雄')\n",
      "label: 0\n",
      " [1.6084627e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是中壢\n",
      "cond_sql: (17, 2, '中壢')\n",
      "label: 0\n",
      " [1.8479867e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是三塊厝\n",
      "cond_sql: (17, 2, '三塊厝')\n",
      "label: 0\n",
      " [3.9016797e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是田中\n",
      "cond_sql: (17, 2, '田中')\n",
      "label: 0\n",
      " [4.0012392e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是臺南\n",
      "cond_sql: (17, 2, '臺南')\n",
      "label: 0\n",
      " [6.926699e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是臺中港\n",
      "cond_sql: (17, 2, '臺中港')\n",
      "label: 0\n",
      " [2.755283e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是八斗子\n",
      "cond_sql: (17, 2, '八斗子')\n",
      "label: 0\n",
      " [1.419102e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是三貂嶺\n",
      "cond_sql: (17, 2, '三貂嶺')\n",
      "label: 0\n",
      " [2.6110988e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是車埕\n",
      "cond_sql: (17, 2, '車埕')\n",
      "label: 0\n",
      " [2.3685118e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: Station是三坑\n",
      "cond_sql: (17, 2, '三坑')\n",
      "label: 0\n",
      " [1.781578e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: DEPTime大于08:00:00\n",
      "cond_sql: (19, 0, '08:00:00')\n",
      "label: 0\n",
      " [0.8214815]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: DEPTime小于08:00:00\n",
      "cond_sql: (19, 1, '08:00:00')\n",
      "label: 0\n",
      " [2.6536281e-05]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: DEPTime是08:00:00\n",
      "cond_sql: (19, 2, '08:00:00')\n",
      "label: 0\n",
      " [3.0555653e-05]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: DEPTime大于03:00:00\n",
      "cond_sql: (19, 0, '03:00:00')\n",
      "label: 0\n",
      " [0.90810615]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: DEPTime小于03:00:00\n",
      "cond_sql: (19, 1, '03:00:00')\n",
      "label: 0\n",
      " [3.9497507e-05]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: DEPTime是03:00:00\n",
      "cond_sql: (19, 2, '03:00:00')\n",
      "label: 0\n",
      " [3.253718e-05]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: DEPTime大于15:00:00\n",
      "cond_sql: (19, 0, '15:00:00')\n",
      "label: 0\n",
      " [0.9767982]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: DEPTime小于15:00:00\n",
      "cond_sql: (19, 1, '15:00:00')\n",
      "label: 0\n",
      " [6.468843e-05]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: DEPTime是15:00:00\n",
      "cond_sql: (19, 2, '15:00:00')\n",
      "label: 0\n",
      " [3.2513697e-05]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是竹北\n",
      "cond_sql: (21, 2, '竹北')\n",
      "label: 0\n",
      " [6.946598e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是嘉北\n",
      "cond_sql: (21, 2, '嘉北')\n",
      "label: 0\n",
      " [1.3461238e-06]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是知本\n",
      "cond_sql: (21, 2, '知本')\n",
      "label: 0\n",
      " [3.7991293e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是三姓橋\n",
      "cond_sql: (21, 2, '三姓橋')\n",
      "label: 0\n",
      " [9.85445e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是北新竹\n",
      "cond_sql: (21, 2, '北新竹')\n",
      "label: 0\n",
      " [4.948902e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是高雄\n",
      "cond_sql: (21, 2, '高雄')\n",
      "label: 0\n",
      " [0.9993925]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是彰化\n",
      "cond_sql: (21, 2, '彰化')\n",
      "label: 0\n",
      " [5.229131e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是中洲\n",
      "cond_sql: (21, 2, '中洲')\n",
      "label: 0\n",
      " [4.0322828e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是臺東\n",
      "cond_sql: (21, 2, '臺東')\n",
      "label: 0\n",
      " [2.7078418e-06]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是八堵\n",
      "cond_sql: (21, 2, '八堵')\n",
      "label: 0\n",
      " [6.243052e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是北埔\n",
      "cond_sql: (21, 2, '北埔')\n",
      "label: 0\n",
      " [7.023926e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是三義\n",
      "cond_sql: (21, 2, '三義')\n",
      "label: 0\n",
      " [1.1107776e-06]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是後龍\n",
      "cond_sql: (21, 2, '後龍')\n",
      "label: 0\n",
      " [6.471274e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是善化\n",
      "cond_sql: (21, 2, '善化')\n",
      "label: 0\n",
      " [4.3796697e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是三民\n",
      "cond_sql: (21, 2, '三民')\n",
      "label: 0\n",
      " [6.1278496e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是中里\n",
      "cond_sql: (21, 2, '中里')\n",
      "label: 0\n",
      " [3.980091e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是臺北\n",
      "cond_sql: (21, 2, '臺北')\n",
      "label: 0\n",
      " [0.00022619]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是北湖\n",
      "cond_sql: (21, 2, '北湖')\n",
      "label: 0\n",
      " [1.3140232e-06]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是臺中\n",
      "cond_sql: (21, 2, '臺中')\n",
      "label: 0\n",
      " [8.7213726e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是後庄\n",
      "cond_sql: (21, 2, '後庄')\n",
      "label: 0\n",
      " [4.6573263e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是後壁\n",
      "cond_sql: (21, 2, '後壁')\n",
      "label: 0\n",
      " [4.3210972e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是桃園\n",
      "cond_sql: (21, 2, '桃園')\n",
      "label: 0\n",
      " [4.958378e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是竹中\n",
      "cond_sql: (21, 2, '竹中')\n",
      "label: 0\n",
      " [7.78356e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是民雄\n",
      "cond_sql: (21, 2, '民雄')\n",
      "label: 0\n",
      " [2.385e-06]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是中壢\n",
      "cond_sql: (21, 2, '中壢')\n",
      "label: 0\n",
      " [4.9526835e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是三塊厝\n",
      "cond_sql: (21, 2, '三塊厝')\n",
      "label: 0\n",
      " [6.6300765e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是田中\n",
      "cond_sql: (21, 2, '田中')\n",
      "label: 0\n",
      " [8.6054814e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是臺南\n",
      "cond_sql: (21, 2, '臺南')\n",
      "label: 0\n",
      " [1.9895376e-06]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是臺中港\n",
      "cond_sql: (21, 2, '臺中港')\n",
      "label: 0\n",
      " [3.264113e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是八斗子\n",
      "cond_sql: (21, 2, '八斗子')\n",
      "label: 0\n",
      " [3.5684997e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是三貂嶺\n",
      "cond_sql: (21, 2, '三貂嶺')\n",
      "label: 0\n",
      " [7.505463e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是車埕\n",
      "cond_sql: (21, 2, '車埕')\n",
      "label: 0\n",
      " [7.3115945e-07]\n",
      "query_id: 0\n",
      "question: 下午三點以後從臺北到高雄的火車有哪些？\n",
      "cond_text: ARRStation是三坑\n",
      "cond_sql: (21, 2, '三坑')\n",
      "label: 0\n",
      " [5.8533556e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是竹北\n",
      "cond_sql: (17, 2, '竹北')\n",
      "label: 0\n",
      " [1.5891825e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是嘉北\n",
      "cond_sql: (17, 2, '嘉北')\n",
      "label: 0\n",
      " [1.9848723e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是知本\n",
      "cond_sql: (17, 2, '知本')\n",
      "label: 0\n",
      " [1.4471134e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是三姓橋\n",
      "cond_sql: (17, 2, '三姓橋')\n",
      "label: 0\n",
      " [2.9709545e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是北新竹\n",
      "cond_sql: (17, 2, '北新竹')\n",
      "label: 0\n",
      " [2.0163199e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是高雄\n",
      "cond_sql: (17, 2, '高雄')\n",
      "label: 0\n",
      " [3.2751265e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是彰化\n",
      "cond_sql: (17, 2, '彰化')\n",
      "label: 0\n",
      " [0.99942577]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是中洲\n",
      "cond_sql: (17, 2, '中洲')\n",
      "label: 0\n",
      " [1.3458848e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是臺東\n",
      "cond_sql: (17, 2, '臺東')\n",
      "label: 0\n",
      " [1.7795318e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是八堵\n",
      "cond_sql: (17, 2, '八堵')\n",
      "label: 0\n",
      " [1.6377986e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是北埔\n",
      "cond_sql: (17, 2, '北埔')\n",
      "label: 0\n",
      " [1.0861402e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是三義\n",
      "cond_sql: (17, 2, '三義')\n",
      "label: 0\n",
      " [3.8681648e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是後龍\n",
      "cond_sql: (17, 2, '後龍')\n",
      "label: 0\n",
      " [1.9482103e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是善化\n",
      "cond_sql: (17, 2, '善化')\n",
      "label: 0\n",
      " [1.2839806e-06]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是三民\n",
      "cond_sql: (17, 2, '三民')\n",
      "label: 0\n",
      " [2.0341623e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是中里\n",
      "cond_sql: (17, 2, '中里')\n",
      "label: 0\n",
      " [1.8417302e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是臺北\n",
      "cond_sql: (17, 2, '臺北')\n",
      "label: 0\n",
      " [2.1428843e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是北湖\n",
      "cond_sql: (17, 2, '北湖')\n",
      "label: 0\n",
      " [2.5512358e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是臺中\n",
      "cond_sql: (17, 2, '臺中')\n",
      "label: 0\n",
      " [1.3438044e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是後庄\n",
      "cond_sql: (17, 2, '後庄')\n",
      "label: 0\n",
      " [1.5910872e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是後壁\n",
      "cond_sql: (17, 2, '後壁')\n",
      "label: 0\n",
      " [1.7894773e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是桃園\n",
      "cond_sql: (17, 2, '桃園')\n",
      "label: 0\n",
      " [5.7188736e-05]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是竹中\n",
      "cond_sql: (17, 2, '竹中')\n",
      "label: 0\n",
      " [2.4533614e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是民雄\n",
      "cond_sql: (17, 2, '民雄')\n",
      "label: 0\n",
      " [1.4929415e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是中壢\n",
      "cond_sql: (17, 2, '中壢')\n",
      "label: 0\n",
      " [1.5113568e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是三塊厝\n",
      "cond_sql: (17, 2, '三塊厝')\n",
      "label: 0\n",
      " [1.8367767e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是田中\n",
      "cond_sql: (17, 2, '田中')\n",
      "label: 0\n",
      " [2.4021102e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是臺南\n",
      "cond_sql: (17, 2, '臺南')\n",
      "label: 0\n",
      " [2.1133448e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是臺中港\n",
      "cond_sql: (17, 2, '臺中港')\n",
      "label: 0\n",
      " [1.2144291e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是八斗子\n",
      "cond_sql: (17, 2, '八斗子')\n",
      "label: 0\n",
      " [8.405121e-08]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是三貂嶺\n",
      "cond_sql: (17, 2, '三貂嶺')\n",
      "label: 0\n",
      " [1.1090442e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是車埕\n",
      "cond_sql: (17, 2, '車埕')\n",
      "label: 0\n",
      " [2.6484994e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: Station是三坑\n",
      "cond_sql: (17, 2, '三坑')\n",
      "label: 0\n",
      " [1.5339644e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: DEPTime大于08:00:00\n",
      "cond_sql: (19, 0, '08:00:00')\n",
      "label: 0\n",
      " [0.98053396]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: DEPTime小于08:00:00\n",
      "cond_sql: (19, 1, '08:00:00')\n",
      "label: 0\n",
      " [0.00015017]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: DEPTime是08:00:00\n",
      "cond_sql: (19, 2, '08:00:00')\n",
      "label: 0\n",
      " [0.00848526]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: DEPTime大于03:00:00\n",
      "cond_sql: (19, 0, '03:00:00')\n",
      "label: 0\n",
      " [0.35959896]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: DEPTime小于03:00:00\n",
      "cond_sql: (19, 1, '03:00:00')\n",
      "label: 0\n",
      " [1.0855519e-05]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: DEPTime是03:00:00\n",
      "cond_sql: (19, 2, '03:00:00')\n",
      "label: 0\n",
      " [9.133148e-06]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: DEPTime大于15:00:00\n",
      "cond_sql: (19, 0, '15:00:00')\n",
      "label: 0\n",
      " [0.95527554]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: DEPTime小于15:00:00\n",
      "cond_sql: (19, 1, '15:00:00')\n",
      "label: 0\n",
      " [4.4917415e-05]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: DEPTime是15:00:00\n",
      "cond_sql: (19, 2, '15:00:00')\n",
      "label: 0\n",
      " [0.0002773]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是竹北\n",
      "cond_sql: (21, 2, '竹北')\n",
      "label: 0\n",
      " [3.0158128e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是嘉北\n",
      "cond_sql: (21, 2, '嘉北')\n",
      "label: 0\n",
      " [6.332065e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是知本\n",
      "cond_sql: (21, 2, '知本')\n",
      "label: 0\n",
      " [4.275723e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是三姓橋\n",
      "cond_sql: (21, 2, '三姓橋')\n",
      "label: 0\n",
      " [6.13836e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是北新竹\n",
      "cond_sql: (21, 2, '北新竹')\n",
      "label: 0\n",
      " [3.72783e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是高雄\n",
      "cond_sql: (21, 2, '高雄')\n",
      "label: 0\n",
      " [9.170285e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是彰化\n",
      "cond_sql: (21, 2, '彰化')\n",
      "label: 0\n",
      " [0.00015169]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是中洲\n",
      "cond_sql: (21, 2, '中洲')\n",
      "label: 0\n",
      " [3.6449737e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是臺東\n",
      "cond_sql: (21, 2, '臺東')\n",
      "label: 0\n",
      " [6.695447e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是八堵\n",
      "cond_sql: (21, 2, '八堵')\n",
      "label: 0\n",
      " [1.9943511e-06]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是北埔\n",
      "cond_sql: (21, 2, '北埔')\n",
      "label: 0\n",
      " [6.883631e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是三義\n",
      "cond_sql: (21, 2, '三義')\n",
      "label: 0\n",
      " [7.7511004e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是後龍\n",
      "cond_sql: (21, 2, '後龍')\n",
      "label: 0\n",
      " [7.334586e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是善化\n",
      "cond_sql: (21, 2, '善化')\n",
      "label: 0\n",
      " [3.1951234e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是三民\n",
      "cond_sql: (21, 2, '三民')\n",
      "label: 0\n",
      " [4.8419196e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是中里\n",
      "cond_sql: (21, 2, '中里')\n",
      "label: 0\n",
      " [3.9503152e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是臺北\n",
      "cond_sql: (21, 2, '臺北')\n",
      "label: 0\n",
      " [6.549991e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是北湖\n",
      "cond_sql: (21, 2, '北湖')\n",
      "label: 0\n",
      " [1.2424703e-06]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是臺中\n",
      "cond_sql: (21, 2, '臺中')\n",
      "label: 0\n",
      " [3.6510968e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是後庄\n",
      "cond_sql: (21, 2, '後庄')\n",
      "label: 0\n",
      " [3.9118657e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是後壁\n",
      "cond_sql: (21, 2, '後壁')\n",
      "label: 0\n",
      " [3.4165097e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是桃園\n",
      "cond_sql: (21, 2, '桃園')\n",
      "label: 0\n",
      " [0.9993185]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是竹中\n",
      "cond_sql: (21, 2, '竹中')\n",
      "label: 0\n",
      " [4.7922777e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是民雄\n",
      "cond_sql: (21, 2, '民雄')\n",
      "label: 0\n",
      " [3.7068074e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是中壢\n",
      "cond_sql: (21, 2, '中壢')\n",
      "label: 0\n",
      " [5.303455e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是三塊厝\n",
      "cond_sql: (21, 2, '三塊厝')\n",
      "label: 0\n",
      " [3.8447774e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是田中\n",
      "cond_sql: (21, 2, '田中')\n",
      "label: 0\n",
      " [6.6781456e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是臺南\n",
      "cond_sql: (21, 2, '臺南')\n",
      "label: 0\n",
      " [5.0061345e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是臺中港\n",
      "cond_sql: (21, 2, '臺中港')\n",
      "label: 0\n",
      " [2.9550824e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是八斗子\n",
      "cond_sql: (21, 2, '八斗子')\n",
      "label: 0\n",
      " [4.002335e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是三貂嶺\n",
      "cond_sql: (21, 2, '三貂嶺')\n",
      "label: 0\n",
      " [4.128331e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是車埕\n",
      "cond_sql: (21, 2, '車埕')\n",
      "label: 0\n",
      " [4.7870117e-07]\n",
      "query_id: 1\n",
      "question: 我想知道彰化到桃園八點發車的有哪幾班？\n",
      "cond_text: ARRStation是三坑\n",
      "cond_sql: (21, 2, '三坑')\n",
      "label: 0\n",
      " [4.079717e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是竹北\n",
      "cond_sql: (17, 2, '竹北')\n",
      "label: 0\n",
      " [1.9318072e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是嘉北\n",
      "cond_sql: (17, 2, '嘉北')\n",
      "label: 0\n",
      " [3.2666978e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是知本\n",
      "cond_sql: (17, 2, '知本')\n",
      "label: 0\n",
      " [1.1395587e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是三姓橋\n",
      "cond_sql: (17, 2, '三姓橋')\n",
      "label: 0\n",
      " [3.0268714e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是北新竹\n",
      "cond_sql: (17, 2, '北新竹')\n",
      "label: 0\n",
      " [1.5018308e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是高雄\n",
      "cond_sql: (17, 2, '高雄')\n",
      "label: 0\n",
      " [3.3034968e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是彰化\n",
      "cond_sql: (17, 2, '彰化')\n",
      "label: 0\n",
      " [3.4370134e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是中洲\n",
      "cond_sql: (17, 2, '中洲')\n",
      "label: 0\n",
      " [1.8928444e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是臺東\n",
      "cond_sql: (17, 2, '臺東')\n",
      "label: 0\n",
      " [7.093842e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是八堵\n",
      "cond_sql: (17, 2, '八堵')\n",
      "label: 0\n",
      " [1.6883605e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是北埔\n",
      "cond_sql: (17, 2, '北埔')\n",
      "label: 0\n",
      " [9.6360765e-08]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是三義\n",
      "cond_sql: (17, 2, '三義')\n",
      "label: 0\n",
      " [5.4213535e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是後龍\n",
      "cond_sql: (17, 2, '後龍')\n",
      "label: 0\n",
      " [1.9381922e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是善化\n",
      "cond_sql: (17, 2, '善化')\n",
      "label: 0\n",
      " [1.099545e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是三民\n",
      "cond_sql: (17, 2, '三民')\n",
      "label: 0\n",
      " [1.6092346e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是中里\n",
      "cond_sql: (17, 2, '中里')\n",
      "label: 0\n",
      " [2.018773e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是臺北\n",
      "cond_sql: (17, 2, '臺北')\n",
      "label: 0\n",
      " [4.2507345e-05]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是北湖\n",
      "cond_sql: (17, 2, '北湖')\n",
      "label: 0\n",
      " [1.7910857e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是臺中\n",
      "cond_sql: (17, 2, '臺中')\n",
      "label: 0\n",
      " [0.9984351]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是後庄\n",
      "cond_sql: (17, 2, '後庄')\n",
      "label: 0\n",
      " [1.4572426e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是後壁\n",
      "cond_sql: (17, 2, '後壁')\n",
      "label: 0\n",
      " [1.6531975e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是桃園\n",
      "cond_sql: (17, 2, '桃園')\n",
      "label: 0\n",
      " [1.7573443e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是竹中\n",
      "cond_sql: (17, 2, '竹中')\n",
      "label: 0\n",
      " [2.9716148e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是民雄\n",
      "cond_sql: (17, 2, '民雄')\n",
      "label: 0\n",
      " [1.3375751e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是中壢\n",
      "cond_sql: (17, 2, '中壢')\n",
      "label: 0\n",
      " [1.6836736e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是三塊厝\n",
      "cond_sql: (17, 2, '三塊厝')\n",
      "label: 0\n",
      " [2.7550544e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是田中\n",
      "cond_sql: (17, 2, '田中')\n",
      "label: 0\n",
      " [2.5954037e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是臺南\n",
      "cond_sql: (17, 2, '臺南')\n",
      "label: 0\n",
      " [8.92067e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是臺中港\n",
      "cond_sql: (17, 2, '臺中港')\n",
      "label: 0\n",
      " [1.0968708e-05]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是八斗子\n",
      "cond_sql: (17, 2, '八斗子')\n",
      "label: 0\n",
      " [1.1136483e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是三貂嶺\n",
      "cond_sql: (17, 2, '三貂嶺')\n",
      "label: 0\n",
      " [1.6400227e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是車埕\n",
      "cond_sql: (17, 2, '車埕')\n",
      "label: 0\n",
      " [2.2394866e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: Station是三坑\n",
      "cond_sql: (17, 2, '三坑')\n",
      "label: 0\n",
      " [1.7796658e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: DEPTime大于08:00:00\n",
      "cond_sql: (19, 0, '08:00:00')\n",
      "label: 0\n",
      " [0.93263793]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: DEPTime小于08:00:00\n",
      "cond_sql: (19, 1, '08:00:00')\n",
      "label: 0\n",
      " [0.00010641]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: DEPTime是08:00:00\n",
      "cond_sql: (19, 2, '08:00:00')\n",
      "label: 0\n",
      " [3.0044677e-05]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: DEPTime大于03:00:00\n",
      "cond_sql: (19, 0, '03:00:00')\n",
      "label: 0\n",
      " [0.9797152]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: DEPTime小于03:00:00\n",
      "cond_sql: (19, 1, '03:00:00')\n",
      "label: 0\n",
      " [0.00011809]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: DEPTime是03:00:00\n",
      "cond_sql: (19, 2, '03:00:00')\n",
      "label: 0\n",
      " [5.672418e-05]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: DEPTime大于15:00:00\n",
      "cond_sql: (19, 0, '15:00:00')\n",
      "label: 0\n",
      " [0.9867068]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: DEPTime小于15:00:00\n",
      "cond_sql: (19, 1, '15:00:00')\n",
      "label: 0\n",
      " [0.00013303]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: DEPTime是15:00:00\n",
      "cond_sql: (19, 2, '15:00:00')\n",
      "label: 0\n",
      " [0.00096823]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是竹北\n",
      "cond_sql: (21, 2, '竹北')\n",
      "label: 0\n",
      " [5.642226e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是嘉北\n",
      "cond_sql: (21, 2, '嘉北')\n",
      "label: 0\n",
      " [1.91522e-06]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是知本\n",
      "cond_sql: (21, 2, '知本')\n",
      "label: 0\n",
      " [5.318589e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是三姓橋\n",
      "cond_sql: (21, 2, '三姓橋')\n",
      "label: 0\n",
      " [4.10413e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是北新竹\n",
      "cond_sql: (21, 2, '北新竹')\n",
      "label: 0\n",
      " [3.7812592e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是高雄\n",
      "cond_sql: (21, 2, '高雄')\n",
      "label: 0\n",
      " [1.6164191e-06]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是彰化\n",
      "cond_sql: (21, 2, '彰化')\n",
      "label: 0\n",
      " [8.955208e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是中洲\n",
      "cond_sql: (21, 2, '中洲')\n",
      "label: 0\n",
      " [3.9068738e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是臺東\n",
      "cond_sql: (21, 2, '臺東')\n",
      "label: 0\n",
      " [6.6570374e-06]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是八堵\n",
      "cond_sql: (21, 2, '八堵')\n",
      "label: 0\n",
      " [7.9019344e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是北埔\n",
      "cond_sql: (21, 2, '北埔')\n",
      "label: 0\n",
      " [4.6460224e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是三義\n",
      "cond_sql: (21, 2, '三義')\n",
      "label: 0\n",
      " [5.019458e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是後龍\n",
      "cond_sql: (21, 2, '後龍')\n",
      "label: 0\n",
      " [8.6375644e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是善化\n",
      "cond_sql: (21, 2, '善化')\n",
      "label: 0\n",
      " [5.0459323e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是三民\n",
      "cond_sql: (21, 2, '三民')\n",
      "label: 0\n",
      " [3.1999082e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是中里\n",
      "cond_sql: (21, 2, '中里')\n",
      "label: 0\n",
      " [4.2515987e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是臺北\n",
      "cond_sql: (21, 2, '臺北')\n",
      "label: 0\n",
      " [0.99917454]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是北湖\n",
      "cond_sql: (21, 2, '北湖')\n",
      "label: 0\n",
      " [9.746053e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是臺中\n",
      "cond_sql: (21, 2, '臺中')\n",
      "label: 0\n",
      " [0.00036187]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是後庄\n",
      "cond_sql: (21, 2, '後庄')\n",
      "label: 0\n",
      " [8.1843416e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是後壁\n",
      "cond_sql: (21, 2, '後壁')\n",
      "label: 0\n",
      " [2.2955426e-06]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是桃園\n",
      "cond_sql: (21, 2, '桃園')\n",
      "label: 0\n",
      " [7.6020143e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是竹中\n",
      "cond_sql: (21, 2, '竹中')\n",
      "label: 0\n",
      " [1.5204561e-06]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是民雄\n",
      "cond_sql: (21, 2, '民雄')\n",
      "label: 0\n",
      " [4.7589808e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是中壢\n",
      "cond_sql: (21, 2, '中壢')\n",
      "label: 0\n",
      " [6.2119375e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是三塊厝\n",
      "cond_sql: (21, 2, '三塊厝')\n",
      "label: 0\n",
      " [1.1788349e-06]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是田中\n",
      "cond_sql: (21, 2, '田中')\n",
      "label: 0\n",
      " [2.8891889e-06]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是臺南\n",
      "cond_sql: (21, 2, '臺南')\n",
      "label: 0\n",
      " [2.4051294e-06]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是臺中港\n",
      "cond_sql: (21, 2, '臺中港')\n",
      "label: 0\n",
      " [3.389154e-06]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是八斗子\n",
      "cond_sql: (21, 2, '八斗子')\n",
      "label: 0\n",
      " [3.4766524e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是三貂嶺\n",
      "cond_sql: (21, 2, '三貂嶺')\n",
      "label: 0\n",
      " [9.209592e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是車埕\n",
      "cond_sql: (21, 2, '車埕')\n",
      "label: 0\n",
      " [7.60659e-07]\n",
      "query_id: 2\n",
      "question: 凌晨3點出發，從臺中到臺北？\n",
      "cond_text: ARRStation是三坑\n",
      "cond_sql: (21, 2, '三坑')\n",
      "label: 0\n",
      " [7.8669257e-07]\n"
     ]
    }
   ],
   "source": [
    "task2_result = merge_result(te_qc_pairs, te_result, threshold=0.97)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "related-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_file = '6.json'\n",
    "with open(final_output_file, 'w') as f:\n",
    "    for query_id, pred_sql in enumerate(task1_result):\n",
    "        cond = list(task2_result.get(query_id, []))\n",
    "        pred_sql['conds'] = cond\n",
    "        json_str = json.dumps(pred_sql, ensure_ascii=False)\n",
    "        f.write(json_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"sql\":{ \n",
    "        \"agg\": [6, 6, 6, 6, 0, 6], \n",
    "        \"cond_conn_op\": 0, \n",
    "        \"conds_ops\": [4, 2, 2, 4, 4, 4],\n",
    "        \"conds_vals\": [Null, '臺北', '臺中', Null, Null, Null]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"sql\":{ \n",
    "        \"sel\": [4],\n",
    "        \"agg\": [0], \n",
    "        \"cond_conn_op\":\n",
    "        \"conds\": [\n",
    "            [1, 2, \"臺北\"],\n",
    "            [2, 2, \"臺中\"]\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-hotel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-control",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
